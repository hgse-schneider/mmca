digraph world {
        size="10,10";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"product" [href="index.svg"];
	"Focused gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Together gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Physiological linkage" [href="https://scholar.google.com/scholar?hl=en&q=Physiological%20Linkage%20of%20Dyadic%20Gaming%20Experience" target="_blank" , color="5"];
	"Group Participation Speaking Cues (Speaking Length, Speaking Turns, Successful Interruptions, Unsuccessful Interruptions, Backchannels" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Speaking Distribution Cues (Speaking Length Skew, Speaking Turns Skew, Successful Interruption Skew, Unsuccessful Interruptions Skew, Backchannels Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Individual Visual Focus of Attention" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"linguistic features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"linguistic features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"perceptual with-me-ness (gaze" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"conceptual with-me-ness (gaze" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"gaze similarity" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"gaze fixations" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"gaze saccades" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"clustered hand/wrist movement" [href="https://scholar.google.com/scholar?hl=en&q=(Dis)Engagement%20Maters:%20Identifying%20Efficacious%20Learning%20Practices%20with%20Multimodal%20Learning%20Analytics" target="_blank" , color="3"];
	"(not" [href="https://scholar.google.com/scholar?hl=en&q=Dual%20Gaze%20as%20a%20Proxy%20for%20Collaboration%20in%20Informal%20Learning" target="_blank" , color="6"];
	"(not" [href="https://scholar.google.com/scholar?hl=en&q=Dual%20Gaze%20as%20a%20Proxy%20for%20Collaboration%20in%20Informal%20Learning" target="_blank" , color="6"];
	"Dialogue episodes (description, management" [href="https://scholar.google.com/scholar?hl=en&q=Dual%20Gaze%20as%20a%20Proxy%20for%20Collaboration%20in%20Informal%20Learning" target="_blank" , color="1"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"events" [href="https://scholar.google.com/scholar?hl=en&q=Analysing%20frequent%20sequential%20patterns%20of%20collaborative%20learning%20activity%20around%20an%20interactive%20tabletop" target="_blank" , color="2"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Convergence measures" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Coherence metrics" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"N-grams" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"Cosine similarity scores" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"Convergence measures" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"Coherence metrics" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="2"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Detecting%20Collaborative%20Dynamics%20Using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"Calculator Use" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Total Movement" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="3"];
	"Distance from the center of the table" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="3"];
	"Number of interventions" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Total speech duration" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times numbers were mentioned" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times mathematical terms were mentioned" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times commands were pronounced" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Total number of pen strokes" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average number of points" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke time length" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke path length" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke displacement" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke pressure" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"transition probability between types of vocalisations" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20identification%20of%20experts%20and%20performance%20prediction%20in%20the%20multimodal%20math%20data%20corpus%20through%20analysis%20of%20speech%20interaction." target="_blank" , color="1"];
	"amount of exploration" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="2"];
	"types of exploration" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="2"];
	"amount of movement" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"type of movement" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"Body synchronization" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"Body distance" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"amount of exploration" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="2"];
	"types of exploration" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="2"];
	"amount of movement" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"type of movement" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"Body synchronization" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"Body distance" [href="https://scholar.google.com/scholar?hl=en&q=Unraveling%20Students'%20Interaction%20around%20a%20Tangible%20Interface%20Using%20Multimodal%20Learning%20Analytics." target="_blank" , color="3"];
	"physiological synchrony (PC" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20the%20relationship%20between%20existing%20and%20new%20measures%20of%20physiological%20synchrony%20and%20collaborative%20learning:%20a%20mixed%20methods%20study" target="_blank" , color="5"];
	"cycles of physiological synchrony (PC" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20the%20relationship%20between%20existing%20and%20new%20measures%20of%20physiological%20synchrony%20and%20collaborative%20learning:%20a%20mixed%20methods%20study" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="1"];
	"Face and upper body movement" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="3"];
	"Galvanic skin response" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Mobile%20Eye-Trackers%20to%20Unpack%20the%20Perceptual%20Benefits%20of%20a%20Tangible%20User%20Interface%20for%20Collaborative%20Learning" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Mobile%20Eye-Trackers%20to%20Unpack%20the%20Perceptual%20Benefits%20of%20a%20Tangible%20User%20Interface%20for%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"convergence (of linguistic styles" [href="https://scholar.google.com/scholar?hl=en&q=Does%20Seeing%20One%20Another’s%20Gaze%20Affect%20Group%20Dialogue?" target="_blank" , color="1"];
	"coherence" [href="https://scholar.google.com/scholar?hl=en&q=Does%20Seeing%20One%20Another’s%20Gaze%20Affect%20Group%20Dialogue?" target="_blank" , color="1"];
	"simple linguistic features" [href="https://scholar.google.com/scholar?hl=en&q=Does%20Seeing%20One%20Another’s%20Gaze%20Affect%20Group%20Dialogue?" target="_blank" , color="1"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="1"];
	"SM - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"IDM - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"SM - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"IDM - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR low frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR high frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR low frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR high frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"EVT of spatial entropy" [href="https://scholar.google.com/scholar?hl=en&q=An%20Alternate%20Statistical%20Lens%20to%20Look%20at%20Collaboration%20Data:%20Extreme%20Value%20Theory" target="_blank" , color="6"];
	"EVT of spatial entropy" [href="https://scholar.google.com/scholar?hl=en&q=An%20Alternate%20Statistical%20Lens%20to%20Look%20at%20Collaboration%20Data:%20Extreme%20Value%20Theory" target="_blank" , color="6"];
	"Speaking turn features [x4]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Acoustic features [x13]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Head motion features [x5]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="4"];
	"Linguistic features [x300+]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Number of faces looking at screen" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="4"];
	"Mean distance between learners" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean distance between hands" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean hand movement speed" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean audio level" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="1"];
	"Arduino measure of complexity" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active hardware blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active software blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"student work phases" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"Joint movement" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"Joint angle" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"dyad proximity" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="6"];
	"cognitive load (from pupil size" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="6"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Pearson‚Äôs correlation coefficient (PCC" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Fisher‚Äôs z-transform (FZT" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Pearson‚Äôs correlation coefficient (PCC" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Fisher‚Äôs z-transform (FZT" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"type of activity done in task" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="2"];
	"amount of face and body movement" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="3"];
	"target for discussion partner" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="2"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="5"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="5"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="5"];
	"Pearson‚Äôs correlation (PC" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="5"];
	"Speech activity" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="1"];
	"time spent individually" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"time spent as a group" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"transition probabilities between collaborative state" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"transition probabilities between collaborative state" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"Focused gaze" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Together gaze" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Physiological linkage" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Group Participation Speaking Cues (Speaking Length, Speaking Turns, Successful Interruptions, Unsuccessful Interruptions, Backchannels" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" -> "product" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Speaking Distribution Cues (Speaking Length Skew, Speaking Turns Skew, Successful Interruption Skew, Unsuccessful Interruptions Skew, Backchannels Skew" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Individual Visual Focus of Attention" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"speech features" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"linguistic features" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"perceptual with-me-ness (gaze" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"conceptual with-me-ness (gaze" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"gaze similarity" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"gaze fixations" -> "product" [label="", labeltooltip=1, style="solid", penwidth=1];
	"gaze saccades" -> "product" [label="", labeltooltip=1, style="solid", penwidth=1];
	"clustered hand/wrist movement" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"(not" -> "product" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Dialogue episodes (description, management" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Joint visual attention" -> "product" [label="corr", labeltooltip=4, style="solid", penwidth=4];
	"events" -> "product" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Convergence measures" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Coherence metrics" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"N-grams" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Cosine similarity scores" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Calculator Use" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total Movement" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Distance from the center of the table" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Number of interventions" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total speech duration" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times numbers were mentioned" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times mathematical terms were mentioned" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times commands were pronounced" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total number of pen strokes" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average number of points" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke time length" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke path length" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke displacement" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke pressure" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"transition probability between types of vocalisations" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"amount of exploration" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"types of exploration" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"amount of movement" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"type of movement" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Body synchronization" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Body distance" -> "product" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"physiological synchrony (PC" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"cycles of physiological synchrony (PC" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"speech rate" -> "product" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Face and upper body movement" -> "product" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Galvanic skin response" -> "product" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Cycles of collaborative / individual work" -> "product" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"Gaze location" -> "product" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Joint visual Attention" -> "product" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"Cross-Recurrence Quantification Analysis (CRQA" -> "product" [label="glm", labeltooltip=4, style="dotted", penwidth=4];
	"convergence (of linguistic styles" -> "product" [label="corr", labeltooltip=1, style="dotted", penwidth=1];
	"coherence" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"simple linguistic features" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"35 Coh-metrix indices" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"SM - EDA" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"IDM - EDA" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"DA - EDA" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"CC - EDA" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"WC - EDA" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"SM - HR" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"IDM - HR" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"DA - HR" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"CC - HR" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"WC - HR low frequency" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"WC - HR high frequency" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"EVT of spatial entropy" -> "product" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Speaking turn features [x4]" -> "product" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Acoustic features [x13]" -> "product" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Head motion features [x5]" -> "product" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Linguistic features [x300+]" -> "product" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Number of faces looking at screen" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean distance between learners" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean distance between hands" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean hand movement speed" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean audio level" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino measure of complexity" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active hardware blocks" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active software blocks" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active blocks" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"student work phases" -> "product" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"joint visual attention" -> "product" [label="glm", labeltooltip=3, style="solid", penwidth=3];
	"Joint movement" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Joint angle" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"dyad proximity" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"cognitive load (from pupil size" -> "product" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Signal Matching (SM" -> "product" [label="corr", labeltooltip=3, style="dotted", penwidth=3];
	"Directional Agreement (DA" -> "product" [label="corr", labeltooltip=3, style="dotted", penwidth=3];
	"Pearson‚Äôs correlation coefficient (PCC" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"Fisher‚Äôs z-transform (FZT" -> "product" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"Instantaneous Derivative Matching (IDM" -> "product" [label="corr", labeltooltip=3, style="dotted", penwidth=3];
	"type of activity done in task" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"amount of face and body movement" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"target for discussion partner" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Pearson‚Äôs correlation (PC" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Speech activity" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"time spent individually" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"time spent as a group" -> "product" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"transition probabilities between collaborative state" -> "product" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"physiological synchrony" -> "product" [label="glm", labeltooltip=3, style="dotted", penwidth=3];
	"shared gaze" -> "product" [label="glm", labeltooltip=1, style="solid", penwidth=1];

 overlap=false 
 splines = true; 


}