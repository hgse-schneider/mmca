digraph world {
        size="10,10";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"performance" [href="index.svg"];
	"Focused gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Together gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Group Participation Speaking Cues (Speaking Length, Speaking Turns, Successful Interruptions, Unsuccessful Interruptions, Backchannels" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Speaking Distribution Cues (Speaking Length Skew, Speaking Turns Skew, Successful Interruption Skew, Unsuccessful Interruptions Skew, Backchannels Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"Individual Visual Focus of Attention" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="1"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"linguistic features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"linguistic features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="1"];
	"gaze fixations" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"gaze saccades" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"events" [href="https://scholar.google.com/scholar?hl=en&q=Analysing%20frequent%20sequential%20patterns%20of%20collaborative%20learning%20activity%20around%20an%20interactive%20tabletop" target="_blank" , color="2"];
	"Calculator Use" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Total Movement" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="3"];
	"Distance from the center of the table" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="3"];
	"Number of interventions" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Total speech duration" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times numbers were mentioned" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times mathematical terms were mentioned" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Times commands were pronounced" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="1"];
	"Total number of pen strokes" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average number of points" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke time length" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke path length" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke displacement" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"Average stroke pressure" [href="https://scholar.google.com/scholar?hl=en&q=Expertise%20estimation%20based%20on%20simple%20multimodal%20features" target="_blank" , color="2"];
	"transition probability between types of vocalisations" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20identification%20of%20experts%20and%20performance%20prediction%20in%20the%20multimodal%20math%20data%20corpus%20through%20analysis%20of%20speech%20interaction." target="_blank" , color="1"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="1"];
	"Face and upper body movement" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="3"];
	"Galvanic skin response" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Mobile%20Eye-Trackers%20to%20Unpack%20the%20Perceptual%20Benefits%20of%20a%20Tangible%20User%20Interface%20for%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"SM - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"IDM - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - EDA" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"SM - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"IDM - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR low frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR high frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"DA - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"CC - HR" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR low frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"WC - HR high frequency" [href="https://scholar.google.com/scholar?hl=en&q=Shared%20Experiences%20of%20Technology%20and%20Trust:%20An%20Experimental%20Study%20of%20Physiological%20Compliance%20Between%20Active%20and%20Passive%20Users%20in%20Technology-Mediated%20Collaborative%20Encounters" target="_blank" , color="5"];
	"EVT of spatial entropy" [href="https://scholar.google.com/scholar?hl=en&q=An%20Alternate%20Statistical%20Lens%20to%20Look%20at%20Collaboration%20Data:%20Extreme%20Value%20Theory" target="_blank" , color="6"];
	"Speaking turn features [x4]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Acoustic features [x13]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Head motion features [x5]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="4"];
	"Linguistic features [x300+]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="1"];
	"Number of faces looking at screen" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="4"];
	"Mean distance between learners" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean distance between hands" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean hand movement speed" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="3"];
	"Mean audio level" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="1"];
	"Arduino measure of complexity" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active hardware blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active software blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"Arduino active blocks" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"student work phases" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="2"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"Joint movement" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"Joint angle" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"dyad proximity" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="3"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Pearson‚Äôs correlation coefficient (PCC" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Fisher‚Äôs z-transform (FZT" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20collaborative%20learning%20success%20with%20physiological%20coupling%20indices%20based%20on%20electrodermal%20activity" target="_blank" , color="5"];
	"type of activity done in task" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="2"];
	"amount of face and body movement" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="3"];
	"target for discussion partner" [href="https://scholar.google.com/scholar?hl=en&q=Focused%20or%20Stuck%20Together:%20Multimodal%20Patterns%20Reveal%20Triads’%20Performance%20in%20Collaborative%20Problem%20Solving" target="_blank" , color="2"];
	"time spent individually" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"time spent as a group" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"transition probabilities between collaborative state" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"transition probabilities between collaborative state" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Motion%20Sensors%20to%20Understand%20Collaborative%20Interactions%20in%20Digital%20Fabrication%20Labs" target="_blank" , color="3"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"physiological synchrony" [href="https://scholar.google.com/scholar?hl=en&q=What%20does%20physiological%20synchrony%20reveal%20about%20metacognitive%20experiences%20and%20group%20performance?" target="_blank" , color="5"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"Faces looking at screen (FLS" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="4"];
	"Distance between learners (DBL" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="3"];
	"Audio level (AUD" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="1"];
	"Focused gaze" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Together gaze" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Group Participation Speaking Cues (Speaking Length, Speaking Turns, Successful Interruptions, Unsuccessful Interruptions, Backchannels" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Silence and Overlap Cues (Fraction of Silence, Fraction of Nonoverlapped Speech, Fraction of two-people and three-people Overlapped Speech" -> "performance" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Speaking Distribution Cues (Speaking Length Skew, Speaking Turns Skew, Successful Interruption Skew, Unsuccessful Interruptions Skew, Backchannels Skew" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Individual Visual Focus of Attention" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"speech features" -> "performance" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"linguistic features" -> "performance" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"gaze fixations" -> "performance" [label="", labeltooltip=1, style="solid", penwidth=1];
	"gaze saccades" -> "performance" [label="", labeltooltip=1, style="solid", penwidth=1];
	"events" -> "performance" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Calculator Use" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total Movement" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Distance from the center of the table" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Number of interventions" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total speech duration" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times numbers were mentioned" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times mathematical terms were mentioned" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Times commands were pronounced" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Total number of pen strokes" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average number of points" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke time length" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke path length" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke displacement" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Average stroke pressure" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"transition probability between types of vocalisations" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"speech rate" -> "performance" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Face and upper body movement" -> "performance" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Galvanic skin response" -> "performance" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Cycles of collaborative / individual work" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Gaze location" -> "performance" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Joint visual Attention" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Cross-Recurrence Quantification Analysis (CRQA" -> "performance" [label="glm", labeltooltip=4, style="dotted", penwidth=4];
	"SM - EDA" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"IDM - EDA" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"DA - EDA" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"CC - EDA" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"WC - EDA" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"SM - HR" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"IDM - HR" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"DA - HR" -> "performance" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"CC - HR" -> "performance" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"WC - HR low frequency" -> "performance" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"WC - HR high frequency" -> "performance" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"EVT of spatial entropy" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Speaking turn features [x4]" -> "performance" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Acoustic features [x13]" -> "performance" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Head motion features [x5]" -> "performance" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Linguistic features [x300+]" -> "performance" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Number of faces looking at screen" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean distance between learners" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean distance between hands" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean hand movement speed" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Mean audio level" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino measure of complexity" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active hardware blocks" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active software blocks" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Arduino active blocks" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"student work phases" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"joint visual attention" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Joint movement" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Joint angle" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"dyad proximity" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Signal Matching (SM" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Directional Agreement (DA" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Pearson‚Äôs correlation coefficient (PCC" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Fisher‚Äôs z-transform (FZT" -> "performance" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Instantaneous Derivative Matching (IDM" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"type of activity done in task" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"amount of face and body movement" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"target for discussion partner" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"time spent individually" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"time spent as a group" -> "performance" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"transition probabilities between collaborative state" -> "performance" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"physiological synchrony" -> "performance" [label="glm", labeltooltip=3, style="dotted", penwidth=3];
	"shared gaze" -> "performance" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Faces looking at screen (FLS" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Distance between learners (DBL" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Audio level (AUD" -> "performance" [label="ml", labeltooltip=1, style="solid", penwidth=1];

 overlap=false 
 splines = true; 


}