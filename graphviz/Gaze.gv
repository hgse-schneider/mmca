digraph world {
        size="10,10";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"Gaze" [href="index.svg"];
	"Focused gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Together gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Focused gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Together gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Gaze transitions" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"visual attention metrics [8 metrics]" [href="https://scholar.google.com/scholar?hl=en&q=A%20Multimodal-Sensor-Enabled%20Room%20for%20Unobtrusive%20Group%20Meeting%20Analysis" target="_blank" , color="6"];
	"visual attention metrics [8 metrics]" [href="https://scholar.google.com/scholar?hl=en&q=A%20Multimodal-Sensor-Enabled%20Room%20for%20Unobtrusive%20Group%20Meeting%20Analysis" target="_blank" , color="6"];
	"visual attention metrics [8 metrics]" [href="https://scholar.google.com/scholar?hl=en&q=A%20Multimodal-Sensor-Enabled%20Room%20for%20Unobtrusive%20Group%20Meeting%20Analysis" target="_blank" , color="6"];
	"visual attention metrics [8 metrics]" [href="https://scholar.google.com/scholar?hl=en&q=A%20Multimodal-Sensor-Enabled%20Room%20for%20Unobtrusive%20Group%20Meeting%20Analysis" target="_blank" , color="6"];
	"Individual Visual Focus of Attention" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Individual Visual Focus of Attention" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Individual Visual Focus of Attention" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" [href="https://scholar.google.com/scholar?hl=en&q=Linking%20Speaking%20and%20Looking%20Behavior%20Patterns%20with%20Group%20Composition,%20Perception,%20and%20Performance" target="_blank" , color="6"];
	"perceptual with-me-ness (gaze" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"conceptual with-me-ness (gaze" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"gaze similarity" [href="https://scholar.google.com/scholar?hl=en&q=Looking%20AT%20versus%20Looking%20THROUGH:%20A%20Dual%20Eye-tracking%20Study%20in%20MOOC%20Context" target="_blank" , color="6"];
	"gaze fixations" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"gaze saccades" [href="https://scholar.google.com/scholar?hl=en&q=A%20Network%20Analytic%20Approach%20to%20Gaze%20Coordination%20during%20a%20Collaborative%20Task" target="_blank" , color="6"];
	"(not" [href="https://scholar.google.com/scholar?hl=en&q=Dual%20Gaze%20as%20a%20Proxy%20for%20Collaboration%20in%20Informal%20Learning" target="_blank" , color="6"];
	"(not" [href="https://scholar.google.com/scholar?hl=en&q=Dual%20Gaze%20as%20a%20Proxy%20for%20Collaboration%20in%20Informal%20Learning" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Detecting%20Collaborative%20Dynamics%20Using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"Gaze" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20Trait%20Classification%20via%20Co-Occurrent%20Multiparty%20Multimodal%20Event%20Discovery" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Gaze%20quality%20assisted%20automatic%20recognition%20of%20social%20contexts%20in%20collaborative%20Tetris" target="_blank" , color="6"];
	"Gaze saccade" [href="https://scholar.google.com/scholar?hl=en&q=Gaze%20quality%20assisted%20automatic%20recognition%20of%20social%20contexts%20in%20collaborative%20Tetris" target="_blank" , color="6"];
	"Gaze fixation" [href="https://scholar.google.com/scholar?hl=en&q=Gaze%20quality%20assisted%20automatic%20recognition%20of%20social%20contexts%20in%20collaborative%20Tetris" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Gaze location" [href="https://scholar.google.com/scholar?hl=en&q=Can%20Eye%20Help%20You?:%20Effects%20of%20Visualizing%20Eye%20Fixations%20on%20Remote%20Collaboration%20Scenarios%20for%20Physical%20Tasks" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Mobile%20Eye-Trackers%20to%20Unpack%20the%20Perceptual%20Benefits%20of%20a%20Tangible%20User%20Interface%20for%20Collaborative%20Learning" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Mobile%20Eye-Trackers%20to%20Unpack%20the%20Perceptual%20Benefits%20of%20a%20Tangible%20User%20Interface%20for%20Collaborative%20Learning" target="_blank" , color="6"];
	"visual focus of attention features" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20Automatic%20Dominance%20Estimation%20in%20Groups%20From%20Visual%20Attention%20and%20Speaking%20Activity" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Audio-visual (Looking while speaking, Looking while listening, Being looked while speaking, Center of attention while speaking, Visual dominance ratio" [href="https://scholar.google.com/scholar?hl=en&q=Emergent%20leaders%20through%20looking%20and%20speaking:%20from%20audio-visual%20data%20to%20multimodal%20recognition" target="_blank" , color="6"];
	"Audio-visual (Looking while speaking, Looking while listening, Being looked while speaking, Center of attention while speaking, Visual dominance ratio" [href="https://scholar.google.com/scholar?hl=en&q=Emergent%20leaders%20through%20looking%20and%20speaking:%20from%20audio-visual%20data%20to%20multimodal%20recognition" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="6"];
	"Count of faces looking at screen" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Multimodal%20Learning%20Analytics%20to%20Identify%20Aspects%20of%20Collaboration%20in%20Project-Based%20Learning" target="_blank" , color="6"];
	"EVT of spatial entropy" [href="https://scholar.google.com/scholar?hl=en&q=An%20Alternate%20Statistical%20Lens%20to%20Look%20at%20Collaboration%20Data:%20Extreme%20Value%20Theory" target="_blank" , color="6"];
	"EVT of spatial entropy" [href="https://scholar.google.com/scholar?hl=en&q=An%20Alternate%20Statistical%20Lens%20to%20Look%20at%20Collaboration%20Data:%20Extreme%20Value%20Theory" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"visual focus of attention" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="6"];
	"cognitive load (from pupil size" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="6"];
	"visual field of attention on a person features" [href="https://scholar.google.com/scholar?hl=en&q=Detecting%20Emergent%20Leader%20in%20a%20Meeting%20Environment" target="_blank" , color="6"];
	"visual field of attention on a person features" [href="https://scholar.google.com/scholar?hl=en&q=Detecting%20Emergent%20Leader%20in%20a%20Meeting%20Environment" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"attention received per person" [href="https://scholar.google.com/scholar?hl=en&q=Employing%20Social%20Gaze%20and%20Speaking%20Activity%20for%20Automatic%20Determination%20of%20the%20Extraversion%20Trait" target="_blank" , color="6"];
	"attention given by person" [href="https://scholar.google.com/scholar?hl=en&q=Employing%20Social%20Gaze%20and%20Speaking%20Activity%20for%20Automatic%20Determination%20of%20the%20Extraversion%20Trait" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"Focused gaze" -> "Gaze" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Together gaze" -> "Gaze" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Gaze transitions" -> "Gaze" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"visual attention metrics [8 metrics]" -> "Gaze" [label="glm", labeltooltip=4, style="dotted", penwidth=4];
	"Individual Visual Focus of Attention" -> "Gaze" [label="corr", labeltooltip=3, style="solid", penwidth=3];
	"Group Looking Cues (Fraction of People Gaze, Fraction of Convergent Gaze, Fraction of Mutual Gaze, Fraction of Shared Gaze, Gaze Skew" -> "Gaze" [label="corr", labeltooltip=3, style="solid", penwidth=3];
	"perceptual with-me-ness (gaze" -> "Gaze" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"conceptual with-me-ness (gaze" -> "Gaze" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"gaze similarity" -> "Gaze" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"gaze fixations" -> "Gaze" [label="", labeltooltip=1, style="solid", penwidth=1];
	"gaze saccades" -> "Gaze" [label="", labeltooltip=1, style="solid", penwidth=1];
	"(not" -> "Gaze" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Joint visual attention" -> "Gaze" [label="glm", labeltooltip=11, style="dotted", penwidth=11];
	"Gaze" -> "Gaze" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Joint visual Attention" -> "Gaze" [label="corr", labeltooltip=5, style="solid", penwidth=5];
	"Cycles of collaborative / individual work" -> "Gaze" [label="corr", labeltooltip=5, style="solid", penwidth=5];
	"Gaze location" -> "Gaze" [label="glm", labeltooltip=3, style="solid", penwidth=3];
	"Gaze saccade" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Gaze fixation" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"visual focus of attention features" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Cross-Recurrence Quantification Analysis (CRQA" -> "Gaze" [label="glm", labeltooltip=5, style="dotted", penwidth=5];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" -> "Gaze" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Audio-visual (Looking while speaking, Looking while listening, Being looked while speaking, Center of attention while speaking, Visual dominance ratio" -> "Gaze" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Count of faces looking at screen" -> "Gaze" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"EVT of spatial entropy" -> "Gaze" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Network features [+20]" -> "Gaze" [label="corr", labeltooltip=6, style="solid", penwidth=6];
	"joint-visual attention" -> "Gaze" [label="corr", labeltooltip=6, style="solid", penwidth=6];
	"joint visual attention" -> "Gaze" [label="corr", labeltooltip=6, style="solid", penwidth=6];
	"visual focus of attention" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"cognitive load (from pupil size" -> "Gaze" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"visual field of attention on a person features" -> "Gaze" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"shared gaze" -> "Gaze" [label="glm", labeltooltip=6, style="solid", penwidth=6];
	"attention received per person" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"attention given by person" -> "Gaze" [label="ml", labeltooltip=1, style="solid", penwidth=1];

 overlap=false 
 splines = true; 


}