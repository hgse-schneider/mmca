digraph world {
        size="15,15";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"Speech Features" [href="index.svg"];
	"88 GeMAPS acoustic features" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"102 extended GeMAPs acoustic features" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"12 MFCCs" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"88 GeMAPS acoustic features" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"12 MFCCs" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"12 MFCCs" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"88 GeMAPS acoustic features" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"102 extended GeMAPs acoustic features" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"12 MFCCs" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="5"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="5"];
	"speech features" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Group%20Performance%20in%20Task-Based%20Interaction" target="_blank" , color="5"];
	"Pitch" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20Trait%20Classification%20via%20Co-Occurrent%20Multiparty%20Multimodal%20Event%20Discovery" target="_blank" , color="5"];
	"Energy" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20Trait%20Classification%20via%20Co-Occurrent%20Multiparty%20Multimodal%20Event%20Discovery" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"Proximity" [href="https://scholar.google.com/scholar?hl=en&q=Acoustic-Prosodic%20Entrainment%20and%20Rapport%20in%20Collaborative%20Learning%20Dialogues" target="_blank" , color="5"];
	"Convergence" [href="https://scholar.google.com/scholar?hl=en&q=Acoustic-Prosodic%20Entrainment%20and%20Rapport%20in%20Collaborative%20Learning%20Dialogues" target="_blank" , color="5"];
	"Synchrony" [href="https://scholar.google.com/scholar?hl=en&q=Acoustic-Prosodic%20Entrainment%20and%20Rapport%20in%20Collaborative%20Learning%20Dialogues" target="_blank" , color="5"];
	"6 One Vs All Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="5"];
	"6 One Vs All Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="5"];
	"audio energy features" [href="https://scholar.google.com/scholar?hl=en&q=Investigating%20Automatic%20Dominance%20Estimation%20in%20Groups%20From%20Visual%20Attention%20and%20Speaking%20Activity" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"Acoustic features [x13]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="5"];
	"Mean audio level" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="5"];
	"Articulation rate" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"peak slope" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"spectral stationarity" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"peak slope" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"Articulation rate" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"spectral stationarity" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20prediction%20of%20expertise%20and%20leadership%20in%20learning%20groups" target="_blank" , color="5"];
	"Audio level (AUD" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"voice features (4 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="5"];
	"thousands of features prosodic speech" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="5"];
	"prosodic and tone features [many]" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"Audio level (AUD" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="5"];
	"thousands of features prosodic speech" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="5"];
	"thousands of features prosodic speech" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="5"];
	"88 GeMAPS acoustic features" -> "Speech Features" [label="ml", labeltooltip=3, style="solid", penwidth=3];
	"102 extended GeMAPs acoustic features" -> "Speech Features" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"12 MFCCs" -> "Speech Features" [label="ml", labeltooltip=4, style="solid", penwidth=4];
	"speech features" -> "Speech Features" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Pitch" -> "Speech Features" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Energy" -> "Speech Features" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"speech rate" -> "Speech Features" [label="clust", labeltooltip=5, style="dotted", penwidth=5];
	"Proximity" -> "Speech Features" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Convergence" -> "Speech Features" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Synchrony" -> "Speech Features" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"6 One Vs All Features" -> "Speech Features" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"audio energy features" -> "Speech Features" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"1582 audio features (from Emobase" -> "Speech Features" [label="ml", labeltooltip=6, style="solid", penwidth=6];
	"35 Coh-metrix indices" -> "Speech Features" [label="corr", labeltooltip=5, style="solid", penwidth=5];
	"Acoustic features [x13]" -> "Speech Features" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Mean audio level" -> "Speech Features" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Articulation rate" -> "Speech Features" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"peak slope" -> "Speech Features" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"spectral stationarity" -> "Speech Features" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"Audio level (AUD" -> "Speech Features" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"voice features (4 features" -> "Speech Features" [label="corr", labeltooltip=6, style="dotted", penwidth=6];
	"thousands of features prosodic speech" -> "Speech Features" [label="ml", labeltooltip=3, style="solid", penwidth=3];
	"prosodic and tone features [many]" -> "Speech Features" [label="ml", labeltooltip=1, style="solid", penwidth=1];

 overlap=false 
 splines = true; 


}