digraph world {
        size="10,10";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"coordination" [href="index.svg"];
	"Focused gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Together gaze" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Gaze transitions" [href="https://scholar.google.com/scholar?hl=en&q=Understanding%20collaborative%20program%20comprehension:%20Interlacing%20gaze%20and%20dialogues" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Eye-Tracking%20Technology%20to%20Support%20Visual%20Coordination%20in%20Collaborative%20Problem-Solving%20Groups" target="_blank" , color="6"];
	"Joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="6"];
	"Convergence measures" [href="https://scholar.google.com/scholar?hl=en&q=The%20Effect%20of%20Mutual%20Gaze%20Perception%20on%20Students’%20Verbal%20Coordination" target="_blank" , color="3"];
	"Sequences of verbal utterances" [href="https://scholar.google.com/scholar?hl=en&q=Capturing%20and%20analyzing%20verbal%20and%20physical%20collaborative%20learning%20interactions%20at%20an%20enriched%20interactive%20tabletop" target="_blank" , color="5"];
	"Sequences of meaningful actions" [href="https://scholar.google.com/scholar?hl=en&q=Capturing%20and%20analyzing%20verbal%20and%20physical%20collaborative%20learning%20interactions%20at%20an%20enriched%20interactive%20tabletop" target="_blank" , color="3"];
	"physiological synchrony (DA" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20the%20relationship%20between%20existing%20and%20new%20measures%20of%20physiological%20synchrony%20and%20collaborative%20learning:%20a%20mixed%20methods%20study" target="_blank" , color="2"];
	"cycles of physiological synchrony (PC" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20the%20relationship%20between%20existing%20and%20new%20measures%20of%20physiological%20synchrony%20and%20collaborative%20learning:%20a%20mixed%20methods%20study" target="_blank" , color="2"];
	"Joint visual Attention" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"Cycles of collaborative / individual work" [href="https://scholar.google.com/scholar?hl=en&q=Leveraging%20Mobile%20Eye-Trackers%20to%20Capture%20Joint%20Visual%20Attention%20in%20Co-Located%20Collaborative%20Learning" target="_blank" , color="6"];
	"card movements" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"scrolling" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"zooming" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"card movements" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"scrolling" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"zooming" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="3"];
	"1582 audio features (from Emobase" [href="https://scholar.google.com/scholar?hl=en&q=High%20Accuracy%20Detection%20of%20Collaboration%20From%20Log%20Data%20and%20Superficial%20Speech%20Features" target="_blank" , color="5"];
	"Cross-Recurrence Quantification Analysis (CRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" [href="https://scholar.google.com/scholar?hl=en&q=Dynamics%20of%20Visual%20Attention%20in%20Multiparty%20Collaborative%20Problem%20Solving%20using%20Multidimensional%20Recurrence%20Quantification%20Analysis" target="_blank" , color="6"];
	"coherence" [href="https://scholar.google.com/scholar?hl=en&q=Does%20Seeing%20One%20Another’s%20Gaze%20Affect%20Group%20Dialogue?" target="_blank" , color="5"];
	"35 Coh-metrix indices" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="5"];
	"Physical synchrony" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20the%20Quality%20of%20Collaborative%20Problem%20Solving%20Through%20Linguistic%20Analysis%20of%20Discourse" target="_blank" , color="1"];
	"Distance between hands" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Multimodal%20Learning%20Analytics%20to%20Identify%20Aspects%20of%20Collaboration%20in%20Project-Based%20Learning" target="_blank" , color="1"];
	"Distance between hands" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Multimodal%20Learning%20Analytics%20to%20Identify%20Aspects%20of%20Collaboration%20in%20Project-Based%20Learning" target="_blank" , color="1"];
	"Count of faces looking at screen" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Multimodal%20Learning%20Analytics%20to%20Identify%20Aspects%20of%20Collaboration%20in%20Project-Based%20Learning" target="_blank" , color="6"];
	"Distance between hands" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Multimodal%20Learning%20Analytics%20to%20Identify%20Aspects%20of%20Collaboration%20in%20Project-Based%20Learning" target="_blank" , color="1"];
	"Touch patterns" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Collaboration%20Patterns%20on%20an%20Interactive%20Tabletop%20in%20a%20Classroom%20Setting" target="_blank" , color="3"];
	"Total movement across upper body joints and body parts" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Using%20Multi-Modal%20Learning%20Analytics%20to%20Support%20and%20Measure%20Collaboration%20in%20Co-Located%20Dyads" target="_blank" , color="1"];
	"talking time" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Using%20Multi-Modal%20Learning%20Analytics%20to%20Support%20and%20Measure%20Collaboration%20in%20Co-Located%20Dyads" target="_blank" , color="5"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Network features [+20]" [href="https://scholar.google.com/scholar?hl=en&q=Toward%20Collaboration%20Sensing" target="_blank" , color="6"];
	"Faces looking at screen (FLS" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="4"];
	"Distance between learners (DBL" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="1"];
	"Audio level (AUD" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="5"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint-visual attention" [href="https://scholar.google.com/scholar?hl=en&q=Unpacking%20Collaborative%20Learning%20Processes%20during%20Hands-on%20Activities%20using%20Mobile%20Eye-Trackers" target="_blank" , color="6"];
	"joint visual attention" [href="https://scholar.google.com/scholar?hl=en&q=3D%20Tangibles%20Facilitate%20Joint%20Visual%20Attention%20in%20Dyads" target="_blank" , color="6"];
	"Joint movement" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="1"];
	"dyad proximity" [href="https://scholar.google.com/scholar?hl=en&q=Exploring%20Collaboration%20Using%20Motion%20Sensors%20and%20Multi-Modal%20Learning%20Analytics" target="_blank" , color="1"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Pearson‚Äôs correlation (PC" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Signal Matching (SM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Instantaneous Derivative Matching (IDM" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Directional Agreement (DA" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"Pearson‚Äôs correlation (PC" [href="https://scholar.google.com/scholar?hl=en&q=Using%20Physiological%20Synchrony%20as%20an%20Indicator%20of%20Collaboration%20Quality,%20Task%20Performance%20and%20Learning" target="_blank" , color="2"];
	"speech time and frequency" [href="https://scholar.google.com/scholar?hl=en&q=An%20Automatic%20Approach%20for%20Mining%20Patterns%20of%20Collaboration%20around%20an%20Interactive%20Tabletop" target="_blank" , color="5"];
	"symmetry of speech among group" [href="https://scholar.google.com/scholar?hl=en&q=An%20Automatic%20Approach%20for%20Mining%20Patterns%20of%20Collaboration%20around%20an%20Interactive%20Tabletop" target="_blank" , color="5"];
	"total number of touch actions" [href="https://scholar.google.com/scholar?hl=en&q=An%20Automatic%20Approach%20for%20Mining%20Patterns%20of%20Collaboration%20around%20an%20Interactive%20Tabletop" target="_blank" , color="3"];
	"symmetry of touch actions among group" [href="https://scholar.google.com/scholar?hl=en&q=An%20Automatic%20Approach%20for%20Mining%20Patterns%20of%20Collaboration%20around%20an%20Interactive%20Tabletop" target="_blank" , color="3"];
	"upper body agitation" [href="https://scholar.google.com/scholar?hl=en&q=Multi-modal%20Social%20Signal%20Analysis%20for%20Predicting%20Agreement%20in%20Conversation%20Settings" target="_blank" , color="1"];
	"hand agitation" [href="https://scholar.google.com/scholar?hl=en&q=Multi-modal%20Social%20Signal%20Analysis%20for%20Predicting%20Agreement%20in%20Conversation%20Settings" target="_blank" , color="1"];
	"head orientation" [href="https://scholar.google.com/scholar?hl=en&q=Multi-modal%20Social%20Signal%20Analysis%20for%20Predicting%20Agreement%20in%20Conversation%20Settings" target="_blank" , color="4"];
	"speaking time / turns" [href="https://scholar.google.com/scholar?hl=en&q=Multi-modal%20Social%20Signal%20Analysis%20for%20Predicting%20Agreement%20in%20Conversation%20Settings" target="_blank" , color="5"];
	"EDA synchrony" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Collaborative%20Learning%20Quality%20through%20Physiological%20Synchrony%20Recorded%20by%20Wearable%20Biosensors" target="_blank" , color="2"];
	"heart rate sychrony" [href="https://scholar.google.com/scholar?hl=en&q=Predicting%20Collaborative%20Learning%20Quality%20through%20Physiological%20Synchrony%20Recorded%20by%20Wearable%20Biosensors" target="_blank" , color="2"];
	"shared gaze" [href="https://scholar.google.com/scholar?hl=en&q=Effects%20of%20Shared%20Gaze%20on%20Audio-%20Versus%20Text-Based%20Remote%20Collaborations" target="_blank" , color="6"];
	"speech time" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="5"];
	"thousands of features prosodic speech" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="5"];
	"movement of objects including revisiting past actions" [href="https://scholar.google.com/scholar?hl=en&q=Using%20the%20Tablet%20Gestures%20and%20Speech%20of%20Pairs%20of%20Students%20to%20Classify%20Their%20Collaboration" target="_blank" , color="3"];
	"duration of speech by each student" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"duration in which each student was only speaker" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"duration of overlapping speech from pairs of students" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"duration of overlapping speech from all people" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"duration of silence for all people" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"prosodic and tone features [many]" [href="https://scholar.google.com/scholar?hl=en&q=Privacy-Preserving%20Speech%20Analytics%20for%20Automatic%20Assessment%20of%20Student%20Collaboration" target="_blank" , color="5"];
	"speech rate" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="5"];
	"Face and upper body movement" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="1"];
	"Galvanic skin response" [href="https://scholar.google.com/scholar?hl=en&q=Modeling%20Team-level%20Multimodal%20Dynamics%20during%20Multiparty%20Collaboration" target="_blank" , color="2"];
	"Focused gaze" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Together gaze" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Gaze transitions" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Joint visual attention" -> "coordination" [label="glm", labeltooltip=2, style="dotted", penwidth=2];
	"Convergence measures" -> "coordination" [label="glm", labeltooltip=1, style="dotted", penwidth=1];
	"Sequences of verbal utterances" -> "coordination" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Sequences of meaningful actions" -> "coordination" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"physiological synchrony (DA" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"cycles of physiological synchrony (PC" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Joint visual Attention" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Cycles of collaborative / individual work" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"card movements" -> "coordination" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"scrolling" -> "coordination" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"zooming" -> "coordination" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"1582 audio features (from Emobase" -> "coordination" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"Cross-Recurrence Quantification Analysis (CRQA" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Multidimensional Recurrence Quantification Analysis (MdRQA" -> "coordination" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"coherence" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"35 Coh-metrix indices" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Physical synchrony" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Distance between hands" -> "coordination" [label="glm", labeltooltip=3, style="solid", penwidth=3];
	"Count of faces looking at screen" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Touch patterns" -> "coordination" [label="", labeltooltip=1, style="solid", penwidth=1];
	"Total movement across upper body joints and body parts" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"talking time" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Network features [+20]" -> "coordination" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"Faces looking at screen (FLS" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Distance between learners (DBL" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Audio level (AUD" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"joint-visual attention" -> "coordination" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"joint visual attention" -> "coordination" [label="corr", labeltooltip=1, style="solid", penwidth=1];
	"Joint movement" -> "coordination" [label="corr", labeltooltip=1, style="dotted", penwidth=1];
	"dyad proximity" -> "coordination" [label="corr", labeltooltip=1, style="dotted", penwidth=1];
	"Signal Matching (SM" -> "coordination" [label="corr", labeltooltip=2, style="dotted", penwidth=2];
	"Instantaneous Derivative Matching (IDM" -> "coordination" [label="corr", labeltooltip=2, style="dotted", penwidth=2];
	"Pearson‚Äôs correlation (PC" -> "coordination" [label="corr", labeltooltip=2, style="solid", penwidth=2];
	"Directional Agreement (DA" -> "coordination" [label="corr", labeltooltip=2, style="dotted", penwidth=2];
	"speech time and frequency" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"symmetry of speech among group" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"total number of touch actions" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"symmetry of touch actions among group" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"upper body agitation" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"hand agitation" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"head orientation" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"speaking time / turns" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"EDA synchrony" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"heart rate sychrony" -> "coordination" [label="ml", labeltooltip=1, style="dotted", penwidth=1];
	"shared gaze" -> "coordination" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"speech time" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"thousands of features prosodic speech" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"movement of objects including revisiting past actions" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"duration of speech by each student" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"duration in which each student was only speaker" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"duration of overlapping speech from pairs of students" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"duration of overlapping speech from all people" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"duration of silence for all people" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"prosodic and tone features [many]" -> "coordination" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"speech rate" -> "coordination" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Face and upper body movement" -> "coordination" [label="clust", labeltooltip=1, style="dotted", penwidth=1];
	"Galvanic skin response" -> "coordination" [label="clust", labeltooltip=1, style="dotted", penwidth=1];

 overlap=false 
 splines = true; 


}