digraph world {
        size="15,15";
        layout=neato
        graph [fontname = "helvetica"];
        node [fontname = "helvetica", colorscheme=set28];
        edge [fontname = "helvetica", colorscheme=set28];

	"Head" [href="index.svg"];
	"51 Facial Action Units" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="4"];
	"51 Facial Action Units" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="4"];
	"51 Facial Action Units" [href="https://scholar.google.com/scholar?hl=en&q=Automatic%20Recognition%20of%20Affective%20Laughter%20in%20Spontaneous%20Dyadic%20Interactions%20from%20Audiovisual%20Signals" target="_blank" , color="4"];
	"facial expression" [href="https://scholar.google.com/scholar?hl=en&q=The%20Additive%20Value%20of%20Multimodal%20Features%20for%20Predicting%20Engagement,%20Frustration,%20and%20Learning%20during%20Tutoring" target="_blank" , color="4"];
	"facial expression" [href="https://scholar.google.com/scholar?hl=en&q=The%20Additive%20Value%20of%20Multimodal%20Features%20for%20Predicting%20Engagement,%20Frustration,%20and%20Learning%20during%20Tutoring" target="_blank" , color="4"];
	"facial expression" [href="https://scholar.google.com/scholar?hl=en&q=The%20Additive%20Value%20of%20Multimodal%20Features%20for%20Predicting%20Engagement,%20Frustration,%20and%20Learning%20during%20Tutoring" target="_blank" , color="4"];
	"Head motion" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20Trait%20Classification%20via%20Co-Occurrent%20Multiparty%20Multimodal%20Event%20Discovery" target="_blank" , color="4"];
	"22 Intra-Personal Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="4"];
	"22 Intra-Personal Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="4"];
	"9 Dyadic Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="4"];
	"9 Dyadic Features" [href="https://scholar.google.com/scholar?hl=en&q=Personality%20classification%20and%20behaviour%20interpretation:%20An%20approach%20based%20on%20feature%20categories" target="_blank" , color="4"];
	"Facial expression" [href="https://scholar.google.com/scholar?hl=en&q=Going%20beyond%20what%20is%20visible:%20What%20multichannel%20data%20can%20reveal%20about%20interaction%20in%20the%20context%20of%20collaborative%20learning?" target="_blank" , color="4"];
	"Head motion features [x5]" [href="https://scholar.google.com/scholar?hl=en&q=Task-independent%20Multimodal%20Prediction%20of%20Group%20Performance%20Based%20on%20Product%20Dimensions" target="_blank" , color="4"];
	"Number of faces looking at screen" [href="https://scholar.google.com/scholar?hl=en&q=Supervised%20machine%20learning%20in%20multimodal%20learning%20analytics%20for%20estimating%20success%20in%20project-based%20learning" target="_blank" , color="4"];
	"Faces looking at screen (FLS" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="4"];
	"facial features" [href="https://scholar.google.com/scholar?hl=en&q=Real-time%20mutual%20gaze%20perception" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"facial expression features (60 features" [href="https://scholar.google.com/scholar?hl=en&q=Multimodal%20Analysis%20of%20Vocal%20Collaborative%20Search:A%20Public%20Corpus%20and%20Results" target="_blank" , color="4"];
	"smiling synchrony" [href="https://scholar.google.com/scholar?hl=en&q=Physiological%20evidence%20of%20interpersonal%20dynamics%20in%20a%20cooperative%20production%20task" target="_blank" , color="4"];
	"smiling synchrony" [href="https://scholar.google.com/scholar?hl=en&q=Physiological%20evidence%20of%20interpersonal%20dynamics%20in%20a%20cooperative%20production%20task" target="_blank" , color="4"];
	"head orientation" [href="https://scholar.google.com/scholar?hl=en&q=Multi-modal%20Social%20Signal%20Analysis%20for%20Predicting%20Agreement%20in%20Conversation%20Settings" target="_blank" , color="4"];
	"Faces looking at screen (FLS" [href="https://scholar.google.com/scholar?hl=en&q=Estimation%20of%20success%20in%20collaborative%20learning%20based%20on%20multimodal%20learning%20analytics%20features" target="_blank" , color="4"];
	"51 Facial Action Units" -> "Head" [label="ml", labeltooltip=3, style="solid", penwidth=3];
	"facial expression" -> "Head" [label="glm", labeltooltip=3, style="solid", penwidth=3];
	"Head motion" -> "Head" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"22 Intra-Personal Features" -> "Head" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"9 Dyadic Features" -> "Head" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"Facial expression" -> "Head" [label="glm", labeltooltip=1, style="solid", penwidth=1];
	"Head motion features [x5]" -> "Head" [label="clust", labeltooltip=1, style="solid", penwidth=1];
	"Number of faces looking at screen" -> "Head" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"Faces looking at screen (FLS" -> "Head" [label="ml", labeltooltip=2, style="solid", penwidth=2];
	"facial features" -> "Head" [label="ml", labeltooltip=1, style="solid", penwidth=1];
	"facial expression features (60 features" -> "Head" [label="corr", labeltooltip=6, style="solid", penwidth=6];
	"smiling synchrony" -> "Head" [label="glm", labeltooltip=2, style="solid", penwidth=2];
	"head orientation" -> "Head" [label="ml", labeltooltip=1, style="solid", penwidth=1];

 overlap=false 
 splines = true; 


}